Implementing Gradient Descent and Visualizing Linear Regression & Cost Function
Project Overview
This project demonstrates a linear regression model built from scratch using the gradient descent algorithm.
We implement gradient descent to optimize the linear model's parameters and visualize the results, including the linear regression line, cost function minimization, and verification using scipy.stats. 
The project is designed to provide an intuitive understanding of how gradient descent works, its convergence, and performance on a sample dataset.

Key Features:
Gradient Descent Implementation: A custom implementation of gradient descent in Python, without using machine learning libraries.
Linear Regression Visualization: Displaying the best-fit line on the given dataset to visualize the relationship between the variables.
Cost Function Plotting: Graphical visualization of the cost function as the algorithm converges, showing how the loss decreases over iterations.
Verification using scipy.stats: A check against scipy.stats.linregress to verify the correctness of the manually implemented gradient descent results.

File Descriptions:
Linear Regression Gradient Decent: A Jupyter source file contains the core logic for loading data, running the gradient descent algorithm, plotting results.
Verification: A Jupyter sorce file containing the verification of linear regression model by scipy.stats 
ex1data1.txt: A sample dataset containing comma separated X and y values for testing the algorithm.
ex1: The exercise problem statemnent

Future Improvements
Implement polynomial regression.
Extend visualization for 3D data.
Experiment with different optimization algorithms (e.g., stochastic gradient descent).
License
This project is open-source and free to use under the MIT License. Feel free to contribute or suggest improvements!

Author
Created by Sahil Mohril. Feel free to reach out for any queries or collaborations.
